{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VuDA9cPjj3up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "AmZDEFWMGIGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCjX5uL85mG6",
        "outputId": "8ee510b0-3018-4979-c51d-91656fab44fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aamra networks limited\n"
          ]
        }
      ],
      "source": [
        "#Find single company name\n",
        "data_html = requests.get ('https://www.dsebd.org/displayCompany.php?name=AAMRANET')\n",
        "soup = BeautifulSoup(data_html.text, 'html.parser')\n",
        "company_name = soup.find('h2', class_ ='BodyHead topBodyHead')\n",
        "name = company_name.find(\"i\")\n",
        "print(name.text.strip())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find all the company names\n",
        "all_data_html = requests.get ('https://www.dsebd.org/company_listing.php')\n",
        "soup = BeautifulSoup(all_data_html.text, 'html.parser')\n",
        "company_names = []\n",
        "all_company =soup.find_all(\"div\", {\"class\": \"row al-li\"})\n",
        "for company in all_company:\n",
        "  all_company_name = company.text.strip()\n",
        "  company_names.append(all_company_name)\n",
        "print(company_names)"
      ],
      "metadata": {
        "id": "cMporDwCVKCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding Trading code\n",
        "trading_html = requests.get ('https://www.dsebd.org/company_listing.php')\n",
        "soup = BeautifulSoup(trading_html.content, 'html.parser')\n",
        "trading_name = soup.find_all('div', class_ ='BodyContent')\n",
        "for trading in trading_name:\n",
        "  name = trading.find_all(\"a\")\n",
        "  for trading_code in name:\n",
        "    print(trading_code.text.strip())\n"
      ],
      "metadata": {
        "id": "cNTTtoqYbtMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find only all company names\n",
        "\n",
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Initialize the list to store company names\n",
        "company_names = []\n",
        "\n",
        "# Iterate through trading_name\n",
        "for trading in trading_name:\n",
        "    name = trading.find_all(\"a\")\n",
        "    for trading_code in name:\n",
        "        data_html = requests.get('https://www.dsebd.org/displayCompany.php?name=' + trading_code.text.strip())\n",
        "        soup = BeautifulSoup(data_html.text, 'html.parser')\n",
        "        company_name = soup.find('h2', class_='BodyHead topBodyHead')\n",
        "        name = company_name.find(\"i\")\n",
        "        company_names.append(name.text.strip())\n",
        "\n",
        "# Specify the file name and path\n",
        "filename = 'company_names.csv'\n",
        "\n",
        "# Write the data to the CSV file\n",
        "with open(filename, 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the company names to the CSV file\n",
        "    writer.writerow(['Company Name'])\n",
        "    writer.writerows([[name] for name in company_names])\n",
        "\n",
        "print(f\"Company names have been saved in {filename}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xGC6rHeeZPcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find all scrip code\n",
        "for result in trading_name:\n",
        "  code = result.find_all(\"a\")\n",
        "  for name in code:\n",
        "    scrip_url = requests.get ('https://www.dsebd.org/displayCompany.php?name=' + name.text.strip())\n",
        "    soup = BeautifulSoup(scrip_url.text, 'html.parser')\n",
        "    scrip_name = soup.find('tr', class_ ='alt')\n",
        "    individual_scrip_code= scrip_name.find(\"th\").find_next_sibling(\"th\").text.split()[-1]\n",
        "    print(individual_scrip_code)"
      ],
      "metadata": {
        "id": "uxW1t-DanBbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Send a GET request to the website\n",
        "url = 'https://dsebd.org/displayCompany.php?name=AAMRANET'\n",
        "response = requests.get(url)\n",
        "# Create a BeautifulSoup object to parse the HTML\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "# Find the specific information\n",
        "find_sector = soup.find('th', string='Sector')\n",
        "# Finding catagory of the Sector\n",
        "find_catagory = find_sector.find_next_siblings('td')\n",
        "# Remove leading/trailing whitespace\n",
        "for td in find_catagory:\n",
        "    data = td.text.strip()\n",
        "    # Process or print the extracted data as needed\n",
        "    print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_8HJvZMGHZs",
        "outputId": "a6b6ea35-8916-43e7-e0e8-9c3b54feab57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IT Sector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from dateutil.parser import parse\n",
        "\n",
        "share_percentage_info = []\n",
        "url = 'https://dsebd.org/displayCompany.php?name=AAMRANET'\n",
        "response = requests.get(url)\n",
        "# Create a BeautifulSoup object to parse the HTML\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "#find Share holding all info\n",
        "find_share_holding_info = (soup.find_all('td', string = re.compile('Share Holding Percentage')))\n",
        "for info in find_share_holding_info:\n",
        "  #print(info.text.strip)\n",
        "#finding date & time from share holding percentage\n",
        "  # date_time =parse(info.text, fuzzy = True)\n",
        "  # print(date_time)\n",
        "  date_time = parse(info.text, fuzzy = True)\n",
        "  print(date_time.date())\n",
        "  share_percentage = ((info.find_next_sibling()).text).split()\n",
        "\n",
        "\n",
        "  for y in share_percentage:\n",
        "    try:\n",
        "      print(float(y))\n",
        "    except(ValueError):\n",
        "      continue\n",
        "\n",
        "      # Specify the file name and path\n",
        "\n",
        "file_path = '/content/company_details.csv'\n",
        "company_details_df.to_csv(file_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8Zn-PLB6X_O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_in_csv(trading_codes, company_rows, percentage_rows):\n",
        "  with open('companies info.csv', 'w', newline='') as company_file:\n",
        "    writer = csv.writer(company_file)\n",
        "    field = [\"Name\", \"Scrip Code\", \"Sector\", \"URL\"]\n",
        "\n",
        "    writer.writerow(field)\n",
        "    writer.writerows(company_rows)\n",
        "\n",
        "  with open('share_holding_percentage.csv', 'w', newline='') as share_file:\n",
        "    writer = csv.writer(share_file)\n",
        "    columns = [\"Date\", \"Sponsor/Director\", \"Govt\", \"Institute\", \"Foreign\", \"Public\"]\n",
        "\n",
        "    writer.writerow(columns)\n",
        "    writer.writerows(percentage_rows)\n",
        "\n",
        "  df = pd.read_csv('companies info.csv')\n",
        "  df['trading_code'] = pd.Series(trading_codes)\n",
        "  df.to_csv('companies info.csv')\n"
      ],
      "metadata": {
        "id": "gxA5NulA2AAA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}